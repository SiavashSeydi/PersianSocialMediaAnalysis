{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from telethon.sync import TelegramClient\n",
    "import pandas as pd\n",
    "from hazm import Normalizer, WordTokenizer, stopwords_list, Lemmatizer\n",
    "import re\n",
    "import emoji\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from tqdm.auto import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "\n",
    "\n",
    "api_id = 21021126          \n",
    "api_hash = '63b67d261fce0b5167175fc84ceb9ca2'\n",
    "\n",
    "client = TelegramClient(\"session_persian\", api_id, api_hash)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<telethon.client.telegramclient.TelegramClient at 0x2cd2a9e7e20>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await client.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-07-22 21:18:22+00:00</td>\n",
       "      <td>غلامحسین محسنی اژه‌ای، رییس قوه قضاییه جمهوری ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-07-22 20:09:26+00:00</td>\n",
       "      <td>یسرائیل کاتز، وزیر دفاع اسرائیل، در جلسه‌ای با...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-07-22 19:45:10+00:00</td>\n",
       "      <td>شهرام خلدی، پژوهشگر تاریخ خاورمیانه و روابط بی...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-07-22 19:44:26+00:00</td>\n",
       "      <td>سازمان عفو بین‌الملل اعلام کرد حمله اسرائیل به...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-07-22 18:46:13+00:00</td>\n",
       "      <td>حسین آقایی، عضو تحریریه ایران‌اینترنشنال، دربا...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date                                               text\n",
       "0 2025-07-22 21:18:22+00:00  غلامحسین محسنی اژه‌ای، رییس قوه قضاییه جمهوری ...\n",
       "1 2025-07-22 20:09:26+00:00  یسرائیل کاتز، وزیر دفاع اسرائیل، در جلسه‌ای با...\n",
       "2 2025-07-22 19:45:10+00:00  شهرام خلدی، پژوهشگر تاریخ خاورمیانه و روابط بی...\n",
       "3 2025-07-22 19:44:26+00:00  سازمان عفو بین‌الملل اعلام کرد حمله اسرائیل به...\n",
       "4 2025-07-22 18:46:13+00:00  حسین آقایی، عضو تحریریه ایران‌اینترنشنال، دربا..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scraping for Data from a Telgram channel\n",
    "channel_username = 'IranintlTV'   \n",
    "limit = 10000                     \n",
    "keywords = ['جنگ']  \n",
    "\n",
    "all_messages = []\n",
    "\n",
    "async with client:\n",
    "    async for message in client.iter_messages(channel_username, limit=limit):\n",
    "        if message.message:\n",
    "            text = message.message\n",
    "            if any(keyword in text for keyword in keywords):\n",
    "                all_messages.append({\n",
    "                    \"date\": message.date,\n",
    "                    \"text\": text\n",
    "                })\n",
    "\n",
    "df = pd.DataFrame(all_messages)\n",
    "df.to_csv(\"../data/data.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the text for the embedder and the tokenizer\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = emoji.replace_emoji(text, replace='')         \n",
    "    text = re.sub(r\"@\\w+\", \"\", text)                     \n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)           \n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()             \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>غلامحسین محسنی اژه‌ای، رییس قوه قضاییه جمهوری ...</td>\n",
       "      <td>غلامحسین محسنی اژه‌ای، رییس قوه قضاییه جمهوری ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>یسرائیل کاتز، وزیر دفاع اسرائیل، در جلسه‌ای با...</td>\n",
       "      <td>یسرائیل کاتز، وزیر دفاع اسرائیل، در جلسه‌ای با...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>شهرام خلدی، پژوهشگر تاریخ خاورمیانه و روابط بی...</td>\n",
       "      <td>شهرام خلدی، پژوهشگر تاریخ خاورمیانه و روابط بی...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>سازمان عفو بین‌الملل اعلام کرد حمله اسرائیل به...</td>\n",
       "      <td>سازمان عفو بین‌الملل اعلام کرد حمله اسرائیل به...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>حسین آقایی، عضو تحریریه ایران‌اینترنشنال، دربا...</td>\n",
       "      <td>حسین آقایی، عضو تحریریه ایران‌اینترنشنال، دربا...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  غلامحسین محسنی اژه‌ای، رییس قوه قضاییه جمهوری ...   \n",
       "1  یسرائیل کاتز، وزیر دفاع اسرائیل، در جلسه‌ای با...   \n",
       "2  شهرام خلدی، پژوهشگر تاریخ خاورمیانه و روابط بی...   \n",
       "3  سازمان عفو بین‌الملل اعلام کرد حمله اسرائیل به...   \n",
       "4  حسین آقایی، عضو تحریریه ایران‌اینترنشنال، دربا...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  غلامحسین محسنی اژه‌ای، رییس قوه قضاییه جمهوری ...  \n",
       "1  یسرائیل کاتز، وزیر دفاع اسرائیل، در جلسه‌ای با...  \n",
       "2  شهرام خلدی، پژوهشگر تاریخ خاورمیانه و روابط بی...  \n",
       "3  سازمان عفو بین‌الملل اعلام کرد حمله اسرائیل به...  \n",
       "4  حسین آقایی، عضو تحریریه ایران‌اینترنشنال، دربا...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/data.csv\")\n",
    "df[\"clean_text\"] = df[\"text\"].map(clean_text)\n",
    "df[['text', 'clean_text']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A light tokenizer for generating bigrams \n",
    "light_stops = {\"است\", \"می\", \"شود\"}\n",
    "normalizer = Normalizer(\n",
    "    persian_numbers=True,\n",
    "    remove_diacritics=True,\n",
    ")\n",
    "tokenizer = WordTokenizer()\n",
    "lemmatizer  = Lemmatizer()\n",
    "\n",
    "def persian_tokenizer_light(text: str) -> list[str]:\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    text = normalizer.normalize(text)\n",
    "    text = re.sub(r\"[^\\u0600-\\u06FF0-9\\s]\", \" \", text)\n",
    "\n",
    "    tokens = [\n",
    "        lemmatizer.lemmatize(tok)\n",
    "        for tok in tokenizer.tokenize(text)\n",
    "        if tok not in light_stops and len(tok) > 1\n",
    "    ]\n",
    "    cleaned = []\n",
    "    for t in tokens:\n",
    "        if \"#\" in t:           \n",
    "            continue\n",
    "        if t.isdigit():        \n",
    "            continue\n",
    "        if len(t) < 3:         \n",
    "            continue\n",
    "        cleaned.append(t)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceIterator:\n",
    "    def __init__(self, series):\n",
    "        self.series = series  \n",
    "    def __iter__(self):\n",
    "        for txt in self.series:\n",
    "            yield persian_tokenizer_light(txt)\n",
    "\n",
    "sentences = SentenceIterator(df['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenising: 1504it [00:01, 1215.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generating bigrams \n",
    "cached = list(tqdm(sentences, desc=\"Tokenising\"))\n",
    "\n",
    "bigram_model = Phrases(\n",
    "    cached,\n",
    "    min_count=10,\n",
    "    threshold=5,\n",
    "    #scoring='npmi',\n",
    "    delimiter='_',      \n",
    ")\n",
    "bigram_phraser = Phraser(bigram_model)\n",
    "bigram_phraser.save(\"persian_bigram_phraser.gensim\")\n",
    "\n",
    "\n",
    "sentences_bigrams = (bigram_phraser[sent] for sent in cached)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>غلامحسین محسنی اژه‌ای، رییس قوه قضاییه جمهوری ...</td>\n",
       "      <td>[غلامحسین, محسنی_اژه, رییس, قوه_قضاییه, جمهوری...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>یسرائیل کاتز، وزیر دفاع اسرائیل، در جلسه‌ای با...</td>\n",
       "      <td>[یسرائیل, کاتز, وزیر_دفاع, اسرائیل, جلسه, فرما...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>شهرام خلدی، پژوهشگر تاریخ خاورمیانه و روابط بی...</td>\n",
       "      <td>[شهرام_خلدی, پژوهشگر_تاریخ, خاورمیانه, روابط_ب...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>سازمان عفو بین‌الملل اعلام کرد حمله اسرائیل به...</td>\n",
       "      <td>[سازمان, عفو_بین, الملل, اعلام, حمله, اسرائیل,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>حسین آقایی، عضو تحریریه ایران‌اینترنشنال، دربا...</td>\n",
       "      <td>[حسین, آقایی, عضو_تحریریه, ایران_اینترنشنال, ع...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  \\\n",
       "0  غلامحسین محسنی اژه‌ای، رییس قوه قضاییه جمهوری ...   \n",
       "1  یسرائیل کاتز، وزیر دفاع اسرائیل، در جلسه‌ای با...   \n",
       "2  شهرام خلدی، پژوهشگر تاریخ خاورمیانه و روابط بی...   \n",
       "3  سازمان عفو بین‌الملل اعلام کرد حمله اسرائیل به...   \n",
       "4  حسین آقایی، عضو تحریریه ایران‌اینترنشنال، دربا...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [غلامحسین, محسنی_اژه, رییس, قوه_قضاییه, جمهوری...  \n",
       "1  [یسرائیل, کاتز, وزیر_دفاع, اسرائیل, جلسه, فرما...  \n",
       "2  [شهرام_خلدی, پژوهشگر_تاریخ, خاورمیانه, روابط_ب...  \n",
       "3  [سازمان, عفو_بین, الملل, اعلام, حمله, اسرائیل,...  \n",
       "4  [حسین, آقایی, عضو_تحریریه, ایران_اینترنشنال, ع...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final Tokenization\n",
    "base_stops = set(stopwords_list())\n",
    "extra_stops = extra_stops = {\n",
    "    \"است\",\"می\",\"شود\",\"iranintltv\",\"http\",\"https\",\n",
    "    \"jpg\",\"png\",\"video\",\"photo\",\"کانال\",\"تلگرام\",\"ای\",\"های\",\"ها\",\n",
    "    \"ایراناینترنشنال\", \"اینترنشنال\", \"ویدیو\", \"کانال\"\n",
    "}\n",
    "\n",
    "stops = base_stops | extra_stops\n",
    "\n",
    "bigram_phraser  = Phraser.load(\"persian_bigram_phraser.gensim\")\n",
    "\n",
    "def persian_tokenizer(text: str) -> list[str]:\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    \n",
    "    tokens = persian_tokenizer_light(text)          \n",
    "    tokens = bigram_phraser[tokens]                 \n",
    "\n",
    "    tokens = [\n",
    "        t for t in tokens\n",
    "        if t not in stops       \n",
    "        and len(t) >= 3\n",
    "        and not t.isdigit()\n",
    "    ]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "df['tokens'] = df['clean_text'].apply(persian_tokenizer)\n",
    "df[['clean_text', 'tokens']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: غلامحسین محسنی اژه‌ای، رییس قوه قضاییه جمهوری اسلامی، در پاسخ به سوال علی رضوانی، بازجو-خبرنگار صداو‌سیما، درباره نقش ره ...\n",
      "Tokens: ['غلامحسین', 'محسنی', 'اژه', 'رییس', 'قوه', 'قضاییه', 'جمهوری', 'اسلامی', 'پاسخ', 'سوال', 'علی', 'رضوانی', 'بازجو', 'خبرنگار', 'صداو']\n",
      "With bigrams: ['غلامحسین', 'محسنی_اژه', 'رییس', 'قوه_قضاییه', 'جمهوری_اسلامی', 'پاسخ', 'سوال', 'علی', 'رضوانی', 'بازجو', 'خبرنگار', 'صداو', 'سیما', 'درباره', 'نقش']\n"
     ]
    }
   ],
   "source": [
    "sample_doc = df['clean_text'].iloc[0]\n",
    "print(\"Original:\", sample_doc[:120], \"...\")\n",
    "print(\"Tokens:\", persian_tokenizer_light(sample_doc)[:15])\n",
    "print(\"With bigrams:\", bigram_phraser[persian_tokenizer_light(sample_doc)][:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../preprocess/predata.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../preprocess/predata.csv\")\n",
    "\n",
    "df=df[df['clean_text'].str.len() > 20]\n",
    "df = df.drop_duplicates(\"clean_text\") \n",
    "docs = df['clean_text'].dropna().astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1470</td>\n",
       "      <td>0_اسرائیل_جنگ_جمهوری_اسلامی_ایران</td>\n",
       "      <td>[اسرائیل, جنگ, جمهوری_اسلامی, ایران, آمریکا, ح...</td>\n",
       "      <td>[نشست شورای امنیت ملی آمریکا که به بحث و بررسی...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>1_آتش_جنگل_جنگلی_ترکیه</td>\n",
       "      <td>[آتش, جنگل, جنگلی, ترکیه, گسترده, نشان, منطقه,...</td>\n",
       "      <td>[تصاویر هوایی، آتش‌سوزی گسترده‌ای را در نزدیکی...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                               Name  \\\n",
       "0      0   1470  0_اسرائیل_جنگ_جمهوری_اسلامی_ایران   \n",
       "1      1     34             1_آتش_جنگل_جنگلی_ترکیه   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [اسرائیل, جنگ, جمهوری_اسلامی, ایران, آمریکا, ح...   \n",
       "1  [آتش, جنگل, جنگلی, ترکیه, گسترده, نشان, منطقه,...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [نشست شورای امنیت ملی آمریکا که به بحث و بررسی...  \n",
       "1  [تصاویر هوایی، آتش‌سوزی گسترده‌ای را در نزدیکی...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence embeddings \n",
    "embedder = SentenceTransformer(\n",
    "    \"xmanii/maux-gte-persian\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Vectorizing \n",
    "vectorizer = CountVectorizer(\n",
    "    tokenizer=persian_tokenizer,   \n",
    "    token_pattern=None,            \n",
    "    lowercase=False       \n",
    ")\n",
    "\n",
    "# Clustering knobs\n",
    "umap_model = UMAP(n_neighbors=15, min_dist=0.1, metric=\"cosine\")\n",
    "hdb_model  = HDBSCAN(min_cluster_size=10, min_samples=5, prediction_data=True)\n",
    "\n",
    "# Fit\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedder,\n",
    "    vectorizer_model=vectorizer,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdb_model,\n",
    "    min_topic_size=10,\n",
    "    calculate_probabilities=True,\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "topic_model.get_topic_info().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('اسرائیل', 0.07321558290818024),\n",
       " ('جنگ', 0.0650711569512354),\n",
       " ('جمهوری_اسلامی', 0.06190402992594244),\n",
       " ('ایران', 0.05842963513723816),\n",
       " ('آمریکا', 0.03452387705704008),\n",
       " ('حمله', 0.026763040096664432),\n",
       " ('کشور', 0.02385149384545015),\n",
       " ('اعلام', 0.02259518312776282),\n",
       " ('جنگ_روزه', 0.02251486209134566),\n",
       " ('تهران', 0.02201829492406831)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation maximum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtopic_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisualize_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\persian_media\\lib\\site-packages\\bertopic\\_bertopic.py:2442\u001b[0m, in \u001b[0;36mBERTopic.visualize_topics\u001b[1;34m(self, topics, top_n_topics, use_ctfidf, custom_labels, title, width, height)\u001b[0m\n\u001b[0;32m   2409\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Visualize topics, their sizes, and their corresponding words.\u001b[39;00m\n\u001b[0;32m   2410\u001b[0m \n\u001b[0;32m   2411\u001b[0m \u001b[38;5;124;03mThis visualization is highly inspired by LDAvis, a great visualization\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2439\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[0;32m   2440\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2441\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 2442\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mplotting\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisualize_topics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2443\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtopics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtopics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2445\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_n_topics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_n_topics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2446\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_ctfidf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_ctfidf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2447\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2448\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtitle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2449\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2450\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2451\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\persian_media\\lib\\site-packages\\bertopic\\plotting\\_topics.py:102\u001b[0m, in \u001b[0;36mvisualize_topics\u001b[1;34m(topic_model, topics, top_n_topics, use_ctfidf, custom_labels, title, width, height)\u001b[0m\n\u001b[0;32m     98\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m UMAP(n_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhellinger\u001b[39m\u001b[38;5;124m\"\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\u001b[38;5;241m.\u001b[39mfit_transform(\n\u001b[0;32m     99\u001b[0m             embeddings\n\u001b[0;32m    100\u001b[0m         )\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 102\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mUMAP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcosine\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[0;32m    105\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUMAP is required to reduce the embeddings.. Please install it using `pip install umap-learn`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    106\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\persian_media\\lib\\site-packages\\umap\\umap_.py:2935\u001b[0m, in \u001b[0;36mUMAP.fit_transform\u001b[1;34m(self, X, y, ensure_all_finite, **kwargs)\u001b[0m\n\u001b[0;32m   2897\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ensure_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   2898\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit X into an embedded space and return that transformed\u001b[39;00m\n\u001b[0;32m   2899\u001b[0m \u001b[38;5;124;03m    output.\u001b[39;00m\n\u001b[0;32m   2900\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2933\u001b[0m \u001b[38;5;124;03m        Local radii of data points in the embedding (log-transformed).\u001b[39;00m\n\u001b[0;32m   2934\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2935\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, ensure_all_finite, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2936\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   2937\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dens:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\persian_media\\lib\\site-packages\\umap\\umap_.py:2817\u001b[0m, in \u001b[0;36mUMAP.fit\u001b[1;34m(self, X, y, ensure_all_finite, **kwargs)\u001b[0m\n\u001b[0;32m   2813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   2814\u001b[0m     epochs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2815\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epochs_list \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epochs_list \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epochs\n\u001b[0;32m   2816\u001b[0m     )\n\u001b[1;32m-> 2817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_, aux_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_embed_data(\n\u001b[0;32m   2818\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raw_data[index],\n\u001b[0;32m   2819\u001b[0m         epochs,\n\u001b[0;32m   2820\u001b[0m         init,\n\u001b[0;32m   2821\u001b[0m         random_state,  \u001b[38;5;66;03m# JH why raw data?\u001b[39;00m\n\u001b[0;32m   2822\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2823\u001b[0m     )\n\u001b[0;32m   2825\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epochs_list \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2826\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding_list\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m aux_data:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\persian_media\\lib\\site-packages\\umap\\umap_.py:2872\u001b[0m, in \u001b[0;36mUMAP._fit_embed_data\u001b[1;34m(self, X, n_epochs, init, random_state, **kwargs)\u001b[0m\n\u001b[0;32m   2867\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_fit_embed_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, n_epochs, init, random_state, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   2868\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"A method wrapper for simplicial_set_embedding that can be\u001b[39;00m\n\u001b[0;32m   2869\u001b[0m \u001b[38;5;124;03m    replaced by subclasses. Arbitrary keyword arguments can be passed\u001b[39;00m\n\u001b[0;32m   2870\u001b[0m \u001b[38;5;124;03m    through .fit() and .fit_transform().\u001b[39;00m\n\u001b[0;32m   2871\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msimplicial_set_embedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2874\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2875\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2876\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initial_alpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2877\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_a\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2878\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_b\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2879\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepulsion_strength\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2880\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnegative_sample_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2881\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2882\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2883\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2884\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_distance_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2885\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_metric_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2886\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdensmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2887\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_densmap_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2888\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_dens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2889\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_output_distance_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2890\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_output_metric_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2891\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_metric\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meuclidean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ml2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2892\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2893\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtqdm_kwds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtqdm_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2895\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\persian_media\\lib\\site-packages\\umap\\umap_.py:1089\u001b[0m, in \u001b[0;36msimplicial_set_embedding\u001b[1;34m(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, densmap, densmap_kwds, output_dens, output_metric, output_metric_kwds, euclidean_output, parallel, verbose, tqdm_kwds)\u001b[0m\n\u001b[0;32m   1086\u001b[0m n_epochs_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(n_epochs) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(n_epochs, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m n_epochs\n\u001b[0;32m   1088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_epochs_max \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m10\u001b[39m:\n\u001b[1;32m-> 1089\u001b[0m     graph\u001b[38;5;241m.\u001b[39mdata[graph\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m<\u001b[39m (\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(n_epochs_max))] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m   1090\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1091\u001b[0m     graph\u001b[38;5;241m.\u001b[39mdata[graph\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m<\u001b[39m (graph\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(default_epochs))] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\persian_media\\lib\\site-packages\\numpy\\core\\_methods.py:41\u001b[0m, in \u001b[0;36m_amax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_amax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     40\u001b[0m           initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_maximum\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
     ]
    }
   ],
   "source": [
    "topic_model.visualize_topics()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "persian_media",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
